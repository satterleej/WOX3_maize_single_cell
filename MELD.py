#!/usr/bin/env python

#Install meld if you haven't already done so.
#pip install meld

import sys
sys.path.append("/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages")

import pandas as pd
import numpy as np
import graphtools as gt
import phate
#import magic
import scprep
import meld
import cmocean
import sklearn
import scipy
import seaborn as sns
import tempfile
import os
import scanpy as sc
import sklearn.mixture

# setting defaults for matplotlib font sizes
import matplotlib.pyplot as plt
plt.rc('font', size=14)

data = scprep.io.load_csv("~/Documents/Projects/narrowsheath/MELD/expression_data.csv", sparse = True, cell_axis = "row")
meta_data = pd.read_csv("~/Documents/Projects/narrowsheath/MELD/meta_data.csv")
data_pca = pd.read_csv("~/Documents/Projects/narrowsheath/MELD/pca_data.csv")
#PHATE is essentially their in house dimensionality-reduction method. Instead I used the 3D UMAP coordinates generated by Seurat.
data_phate = pd.read_csv("~/Documents/Projects/narrowsheath/MELD/3Dumap_data.csv")
data_phate = data_phate.values #Convert pandas dataframe to numpy array
#data_umap2d = pd.read_csv("~/Documents/Projects/narrowsheath/MELD/2Dumap_data.csv")
#data_umap2d = data_umap2d.values #Convert pandas dataframe to numpy array

adata = data.values #Convert pandas dataframe to numpy array

#Removing lowly detected genes (for genes detected in fewer than 10 cells)
data = scprep.filter.filter_rare_genes(data)

#Normalize by cell library size
data_libnorm, libsize = scprep.normalize.library_size_normalize(data, return_library_size=True)

#Square root transform the data
data_sqrt = np.sqrt(data_libnorm)

#Separating replicate and conditions
meta_data['genotype'] = [1 if sl.startswith('Z') else 0 for sl in meta_data['genotype']]
meta_data['genotype_name'] = ['ns' if g == 1 else 'WT' for g in meta_data['genotype']]
meta_data['replicate'] = [sl[-1] for sl in meta_data['sample_labels']]

#MELD parameter optimization
benchmarker = meld.Benchmarker()
benchmarker.fit_phate = data_phate
benchmarker.data_phate = data_phate


# fig, axes = plt.subplots(1,3, figsize=(12,4))
# 
# for i, ax in enumerate(axes):
#     benchmarker.generate_ground_truth_pdf()
#     scprep.plot.scatter2d(data_phate, c=benchmarker.pdf, cmap=meld.utils.get_meld_cmap(),
#                      vmin=0, vmax=1, ticks=False, ax=ax)
# 
# fig.tight_layout()

from joblib import Parallel, delayed

def simulate_pdf_calculate_likelihood(benchmarker, seed, beta):
    benchmarker.set_seed(seed)
    benchmarker.generate_ground_truth_pdf()
    
    benchmarker.generate_sample_labels()
    benchmarker.calculate_MELD_likelihood(beta=beta)
    MELD_mse = benchmarker.calculate_mse(benchmarker.expt_likelihood)
    return MELD_mse, seed, beta, benchmarker.graph.knn

#Play around with these parameters to find values that give lowest MSE below. May help to do coarse search and narrow down on optimum with a second fine search. 
knn_range = np.arange(3, 9, 1)
beta_range = np.arange(12, 37, 1)
results = []

with Parallel(n_jobs=3) as p:
    for knn in knn_range:
        # doing this outside the parallel loop because building the graph takes the longest
        benchmarker.fit_graph(adata, knn=knn)
        print(knn)
        curr_results = p(delayed(simulate_pdf_calculate_likelihood)(benchmarker, seed, beta) \
                                       for seed in range(25) for beta in beta_range)
        curr_results = pd.DataFrame(curr_results, columns = ['MSE', 'seed', 'beta', 'knn'])
        results.append(curr_results)

results = pd.concat(results, axis=0)

## Determine optimal parameters by calculating MSE
results_wide = results.groupby(['beta', 'knn']).mean().sort_values(by='MSE').reset_index()
top_result = results_wide.sort_values('MSE').iloc[0]


#Run MELD with optimal parameters
G = gt.Graph(data_pca, knn=int(8), use_pygsp=True)
meld_op = meld.MELD(beta=14)
sample_densities = meld_op.fit_transform(G, sample_labels=meta_data['sample_labels'])

def replicate_normalize_densities(sample_densities, replicate):
    replicates = np.unique(replicate)
    sample_likelihoods = sample_densities.copy()
    for rep in replicates:
        curr_cols = sample_densities.columns[[col.endswith(rep) for col in sample_densities.columns]]
        sample_likelihoods[curr_cols] = sklearn.preprocessing.normalize(sample_densities[curr_cols], norm='l1')
    return sample_likelihoods

sample_likelihoods = replicate_normalize_densities(sample_densities, meta_data['replicate'])


#Add likelihood data to meta_data data frame
experimental_samples = ['Z_rep1', 'Z_rep2']
meta_data['ns_likelihood'] = sample_likelihoods[experimental_samples].mean(axis=1).values

#Output MELD results to file
sample_likelihoods.to_csv("~/Documents/Projects/narrowsheath/MELD/sample_likelihoods.txt")
meta_data.to_csv("~/Documents/Projects/narrowsheath/MELD/meta_data_MELD.txt", sep = "\t")





